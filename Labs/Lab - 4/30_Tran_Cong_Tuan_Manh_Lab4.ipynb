{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucVaAlCquP6V"
   },
   "source": [
    "# I. Hướng dẫn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4z7aSqLuP7G"
   },
   "source": [
    "## Khởi tạo Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Classification\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc và load tập dữ liệu Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisDF = (spark.read\n",
    "          .option(\"HEADER\", True)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .csv(\"./data/iris.csv\")\n",
    "         )\n",
    "\n",
    "irisDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuyển cột `class` (kiểu string) thành `label` (kiểu double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|label|\n",
      "+------------+-----------+------------+-----------+-----------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|  0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|  0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|  0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|  0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|  0.0|\n",
      "+------------+-----------+------------+-----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "class_indexer = StringIndexer(inputCol = 'class', outputCol = 'label')\n",
    "\n",
    "irisDFindexed = class_indexer.fit(irisDF).transform(irisDF)\n",
    "\n",
    "irisDFindexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tập dữ liệu Iris\n",
    "\n",
    "`sepal_length`: chiều dài đài hoa (cm)\n",
    "\n",
    "`sepal_width`: chiều rộng đài hoa (cm)\n",
    "\n",
    "`petal_length`: chiều dài cánh hoa (cm)\n",
    "\n",
    "`petal_width`: chiều rộng cánh hoa (cm)\n",
    "\n",
    "`class/label`: loại hoa\n",
    "\n",
    "![Iris dataset](./image/iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chia dữ liệu thành train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDF, testDF) = irisDFindexed.randomSplit([.8, .2], seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xem các loại biến trong tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sepal_length', 'double'),\n",
       " ('sepal_width', 'double'),\n",
       " ('petal_length', 'double'),\n",
       " ('petal_width', 'double'),\n",
       " ('class', 'string'),\n",
       " ('label', 'double')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDFindexed.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biến đổi train data và test data theo định dạng của Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[4.3,3.0,1.1,0.1]|  0.0|\n",
      "|[4.4,2.9,1.4,0.2]|  0.0|\n",
      "|[4.4,3.0,1.3,0.2]|  0.0|\n",
      "|[4.4,3.2,1.3,0.2]|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = ['sepal_length','sepal_width','petal_length','petal_width'],\n",
    "                            outputCol = 'features')\n",
    "assembler_train = assembler.transform(trainDF)\n",
    "\n",
    "X_train = assembler_train.select('features', 'label')\n",
    "X_train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tạo mô hình Logistic Regression\n",
    "\n",
    "Tạo một một hình Logistic Regression và huấn luyện mô hình trên `X_train` với `labelCol` là `'label'` và `featuresCol` là `'features'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "logitModel = logit.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Áp dụng mô hình trên test data\n",
    "\n",
    "Áp dụng biến đổi cho tập test tương tự như trên tập train. In ra vài dòng sau khi biến đổi để xem kết quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[4.5,2.3,1.3,0.3]|  0.0|\n",
      "|[4.8,3.1,1.6,0.2]|  0.0|\n",
      "|[4.8,3.4,1.6,0.2]|  0.0|\n",
      "|[4.8,3.4,1.9,0.2]|  0.0|\n",
      "|[4.9,2.5,4.5,1.7]|  2.0|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler_test = assembler.transform(testDF)\n",
    "X_test = assembler_test.select('features', 'label')\n",
    "X_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dự đoán trên test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  2.0|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = logitModel.transform(X_test)\n",
    "predictions.select(\"prediction\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Đánh giá mô hình\n",
    "\n",
    "Tính giá trị `Accuracy` của mô hình trên tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.961538\n",
      "Test Error = 0.0384615\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Tạo ML pipeline và đánh giá dùng phương pháp cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'features=[4.5,2.3,1.3,0.3], label=2.0 -> prediction=2.0'\n",
      "'features=[4.8,3.1,1.6,0.2], label=2.0 -> prediction=2.0'\n",
      "'features=[4.8,3.4,1.6,0.2], label=2.0 -> prediction=2.0'\n",
      "'features=[4.8,3.4,1.9,0.2], label=2.0 -> prediction=2.0'\n",
      "'features=[4.9,2.5,4.5,1.7], label=1.0 -> prediction=0.0'\n",
      "Test Error = 0.0384615\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "logit = LogisticRegression(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "# Define indexer\n",
    "indexer = StringIndexer(inputCol = 'class', \n",
    "                        outputCol = 'label')\n",
    "\n",
    "# Define assembler\n",
    "assembler = VectorAssembler(inputCols = ['sepal_length','sepal_width','petal_length','petal_width'],\n",
    "                            outputCol = 'features')\n",
    "\n",
    "# Configure an ML pipeline, which consists of two stages: indexer, assembler, and logit.\n",
    "pipeline = Pipeline(stages = [indexer, assembler, logit])\n",
    "\n",
    "# Specify evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"label\", \n",
    "    predictionCol = \"prediction\",\n",
    "    metricName = \"accuracy\"\n",
    ")\n",
    "\n",
    "# Specify parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(logit.regParam , [0.01, 0.1, 1])\n",
    "            .build())\n",
    "\n",
    "# Train/test split\n",
    "(trainDF, testDF) = irisDF.randomSplit([.8, .2], seed = 1)\n",
    "\n",
    "# Setup CrossValidator \n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "cv = CrossValidator(estimator = logit, \n",
    "                    evaluator = evaluator, \n",
    "                    estimatorParamMaps = paramGrid, \n",
    "                    numFolds = 3, \n",
    "                    parallelism = 2, \n",
    "                    seed = 1)\n",
    "\n",
    "# Run cross-validation on training data, and choose the best set of parameters\n",
    "logitModel = pipeline.fit(trainDF)\n",
    "\n",
    "# Make predictions on test data. logitModel uses the best model found (regParam = 0.1)\n",
    "prediction = logitModel.transform(testDF)\n",
    "result = prediction.select(\"features\", \"label\", \"prediction\").collect()\n",
    "\n",
    "# Print some predictions\n",
    "for row in result[0:5]:\n",
    "    pp.pprint(\"features=%s, label=%s -> prediction=%s\" % \n",
    "              (row.features, row.label, row.prediction))\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Áp dụng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Câu 1 - Áp dụng `LogisticRegression` với tập dữ liệu `Auto`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu hỏi này sử dụng Logistic Regression trên tập dữ liệu `Auto` để dự đoán một xe cho trước có `mpg` là `high` hay `low`.\n",
    "\n",
    "**Auto Data Set Description**\n",
    "\n",
    "A data frame with 392 observations on the following 9 variables.\n",
    "\n",
    "- `mpg`: miles per gallon\n",
    "\n",
    "- `cylinders`: Number of cylinders between 4 and 8\n",
    "\n",
    "- `displacement`: Engine displacement (cu. inches)\n",
    "\n",
    "- `horsepower`: Engine horsepower\n",
    "\n",
    "- `weight`: Vehicle weight (lbs.)\n",
    "\n",
    "- `acceleration`: Time to accelerate from 0 to 60 mph (sec.)\n",
    "\n",
    "- `year`: Model year (modulo 100)\n",
    "\n",
    "- `origin`: Origin of car (1. American, 2. European, 3. Japanese)\n",
    "\n",
    "- `name`: Vehicle name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.** Tạo một binary variable nhận giá trị 1 (`high`) với các xe có `mpg` lớn hơn median mpg, và nhận giá trị 0 (`low`) cho các xe còn lại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đọc dữ liệu từ file Auto.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|year|origin|                name|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "|18.0|        8|       307.0|       130|  3504|        12.0|  70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|       165|  3693|        11.5|  70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0|       150|  3436|        11.0|  70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0|       150|  3433|        12.0|  70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0|       140|  3449|        10.5|  70|     1|         ford torino|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AutoDF = (spark.read\n",
    "          .option(\"HEADER\", True)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .csv(\"./data/Auto.csv\")\n",
    "         )\n",
    "\n",
    "AutoDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tính median và tính high, low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+----+------+--------------------+-----+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|year|origin|                name|label|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+-----+\n",
      "|18.0|        8|       307.0|       130|  3504|        12.0|  70|     1|chevrolet chevell...|    1|\n",
      "|15.0|        8|       350.0|       165|  3693|        11.5|  70|     1|   buick skylark 320|    0|\n",
      "|18.0|        8|       318.0|       150|  3436|        11.0|  70|     1|  plymouth satellite|    1|\n",
      "|16.0|        8|       304.0|       150|  3433|        12.0|  70|     1|       amc rebel sst|    0|\n",
      "|17.0|        8|       302.0|       140|  3449|        10.5|  70|     1|         ford torino|    0|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Calculate the median of `mpg`\n",
    "median = AutoDF.approxQuantile('mpg', [0.5], 0.25)[0]\n",
    "\n",
    "# Define the function to return 0 or 1 based on median of `mpg` attribbute\n",
    "def alternate_with_med(x):\n",
    "    return 0 if x < median else 1\n",
    "\n",
    "# Call function and load into DataFrame with new column name `label`\n",
    "Alter_udf = udf(alternate_with_med, IntegerType())\n",
    "AutoDF = AutoDF.withColumn('label', Alter_udf(AutoDF['mpg']))\n",
    "\n",
    "AutoDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.** Áp dụng Logistic Regression cho tập dữ liệu với các giá trị siêu tham số `regParam` khác nhau để dự đoán `mpg`. Cho biết cross-validation error ứng với các giá trị khác nhau của siêu tham số này. Nhận xét kết quả thu được. Tham khảo document về Logistic Regression của Spark ở [LogisticRegression](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html#pyspark.ml.classification.LogisticRegression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tạo ML pipeline dùng phương pháp Cross Validation, ParamGrid thử các siêu tham số khác nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'features=[8.0,350.0,3664.0,11.0,73.0,1.0], label=0 -> prediction=0.0'\n",
      "'features=[8.0,302.0,3169.0,12.0,75.0,1.0], label=0 -> prediction=1.0'\n",
      "'features=[8.0,302.0,3870.0,15.0,76.0,1.0], label=0 -> prediction=0.0'\n",
      "'features=[8.0,302.0,4294.0,16.0,72.0,1.0], label=0 -> prediction=0.0'\n",
      "'features=[8.0,318.0,3755.0,14.0,76.0,1.0], label=0 -> prediction=0.0'\n",
      "Accuracy = 0.894737\n",
      "Test Error = 0.105263\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "logit = LogisticRegression(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "# Define assembler\n",
    "assembler = VectorAssembler(inputCols = ['cylinders','displacement', 'weight','acceleration','year','origin'],\n",
    "                            outputCol = 'features')\n",
    "\n",
    "# Configure an ML pipeline, which consists of two stages: indexer, assembler, and logit.\n",
    "pipeline = Pipeline(stages = [ assembler, logit])\n",
    "\n",
    "# Specify evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"label\", \n",
    "    predictionCol = \"prediction\",\n",
    "    metricName = \"accuracy\"\n",
    ")\n",
    "\n",
    "# Specify parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(logit.regParam , [0.01, 0.05, 0.1, 0.5, 1])\n",
    "            .build())\n",
    "\n",
    "# Train/test split\n",
    "(trainDF, testDF) = AutoDF.randomSplit([.8, .2], seed = 1)\n",
    "\n",
    "# Setup CrossValidator \n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "cv = CrossValidator(estimator = pipeline, \n",
    "                    evaluator = evaluator, \n",
    "                    estimatorParamMaps = paramGrid, \n",
    "                    numFolds = 3, \n",
    "                    parallelism = 2, \n",
    "                    seed = 1)\n",
    "\n",
    "# Run cross-validation on training data, and choose the best set of parameters\n",
    "logitModel = cv.fit(trainDF)\n",
    "\n",
    "# Make predictions on test data. logitModel uses the best model found (regParam = 0.1)\n",
    "logistic_prediction = logitModel.transform(testDF)\n",
    "result = logistic_prediction.select(\"features\", \"label\", \"prediction\").collect()\n",
    "\n",
    "# Print some predictions\n",
    "for row in result[0:5]:\n",
    "    pp.pprint(\"features=%s, label=%s -> prediction=%s\" % \n",
    "              (row.features, row.label, row.prediction))\n",
    "\n",
    "accuracy = evaluator.evaluate(logistic_prediction)\n",
    "\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nhận xét:\n",
    "Accuracy của mô hình Logistic Regression với các tham số như trên là: 0.894737, tương đương khoảng  89.4737%, một mức độ chính xác có thể xem là khá cao."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8I0zkl9uP7o"
   },
   "source": [
    "## Câu 2 - So sánh các mô hình phân loại"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kKDfGgFuP7p"
   },
   "source": [
    "Thực hiện việc train tất cả các mô hình `LogisticRegression`, `DecisionTreeClassifier` và `RandomForestClassifier`, `GBTClassifier`, `MultilayerPerceptronClassifier`, `LinearSVC`, `NaiveBayes` trên tập dữ liệu HeartDisease (https://archive.ics.uci.edu/ml/datasets/heart+Disease). Điều chỉnh các siêu tham số của các mô hình để chọn mô hình tốt nhất dùng cross validation. So sánh và nhận xét về kết quả của các mô hình. Để tránh lặp lại các bước xử lý giống nhau nhiều lần như ở trên bạn nên tạo pipeline các bước xử lý. Tham khảo cách tạo pipeline cho mô hình ở [đây](https://spark.apache.org/docs/latest/ml-pipeline.html). Tham khảo document về các classifier của Spark ở [classification module](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#classification).\n",
    "\n",
    "Bên dưới là một số gợi ý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "fcjO6ceEuP7p",
    "outputId": "5c27fe50-8b51-4f9f-eb18-5c9f9d945cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+------------+------+----+---+-------+-----+-----+-------+-----+---+----------+---+\n",
      "|_c0|Age|Sex|   ChestPain|RestBP|Chol|Fbs|RestECG|MaxHR|ExAng|Oldpeak|Slope| Ca|      Thal|AHD|\n",
      "+---+---+---+------------+------+----+---+-------+-----+-----+-------+-----+---+----------+---+\n",
      "|  1| 63|  1|     typical|   145| 233|  1|      2|  150|    0|    2.3|    3|  0|     fixed| No|\n",
      "|  2| 67|  1|asymptomatic|   160| 286|  0|      2|  108|    1|    1.5|    2|  3|    normal|Yes|\n",
      "|  3| 67|  1|asymptomatic|   120| 229|  0|      2|  129|    1|    2.6|    2|  2|reversable|Yes|\n",
      "|  4| 37|  1|  nonanginal|   130| 250|  0|      0|  187|    0|    3.5|    3|  0|    normal| No|\n",
      "|  5| 41|  0|  nontypical|   130| 204|  0|      2|  172|    0|    1.4|    1|  0|    normal| No|\n",
      "+---+---+---+------------+------+----+---+-------+-----+-----+-------+-----+---+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "heart = (spark.read\n",
    "          .option(\"HEADER\", True)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .csv(\"./data/HeartDisease.csv\")\n",
    "         )\n",
    "\n",
    "heart.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "gjXd2ufGuP7q",
    "outputId": "71973970-8977-4233-a189-cacb1adaec78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "y5rPpSAvuP7r",
    "outputId": "41308241-e694-4d92-fa87-f1ba964b1988"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(heart.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "vig7uk4QuP7s",
    "outputId": "05c11c0d-0ffb-4d0f-ba31-3f65587d82ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('Age', 'int'),\n",
       " ('Sex', 'int'),\n",
       " ('ChestPain', 'string'),\n",
       " ('RestBP', 'int'),\n",
       " ('Chol', 'int'),\n",
       " ('Fbs', 'int'),\n",
       " ('RestECG', 'int'),\n",
       " ('MaxHR', 'int'),\n",
       " ('ExAng', 'int'),\n",
       " ('Oldpeak', 'double'),\n",
       " ('Slope', 'int'),\n",
       " ('Ca', 'string'),\n",
       " ('Thal', 'string'),\n",
       " ('AHD', 'string')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Add new colummn with Label to classification\n",
    "heart = heart.withColumn(\"label\", when(heart.AHD == \"No\", 0)\n",
    "                                 .when(heart.AHD == \"Yes\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "UlA0RML6uP7t",
    "outputId": "76ba8c2d-73b3-4fdd-8f27-86b54e297208"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, Age: int, Sex: int, ChestPain: string, RestBP: int, Chol: int, Fbs: int, RestECG: int, MaxHR: int, ExAng: int, Oldpeak: double, Slope: int, Ca: int, Thal: string, AHD: string, label: int]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart.withColumn('Ca', heart.Ca.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipiline for model  \n",
    "\n",
    "Build pipeline with cross validation, paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "oCQg2S_GuP7t",
    "outputId": "3bfe0853-198d-4c26-a5aa-b94acf69f47a"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "catInputCols = ['ChestPain', 'RestECG', 'Slope', 'Thal']\n",
    "catOutputCols = [x + \"Index\" for x in catInputCols]\n",
    "oheCatOutputCols = [x + \"OHE\" for x in catInputCols]\n",
    "\n",
    "catIndexer = StringIndexer(inputCols = catInputCols, outputCols = catOutputCols)\n",
    "\n",
    "catOHEncoder = OneHotEncoder(inputCols = catOutputCols, \n",
    "                          outputCols = oheCatOutputCols)\n",
    "\n",
    "# Define assembler\n",
    "assembler = VectorAssembler(inputCols = ['ChestPainOHE','RestECGOHE','SlopeOHE','Age','Sex',\n",
    "                                         'RestBP','Chol','Fbs','MaxHR','ExAng','Oldpeak'],\n",
    "                            outputCol = 'features')\n",
    "\n",
    "# Train/test split\n",
    "(trainDF, testDF) = heart.randomSplit([.8, .2], seed = 1)\n",
    "\n",
    "# view the transformed data\n",
    "# transformed_heart = pipelineModel.transform(heart)\n",
    "# transformed_heart.select(catInputCols + oheCatOutputCols).show(5)\n",
    "\n",
    "# Specify evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"label\", \n",
    "    predictionCol = \"prediction\",\n",
    "    metricName = \"accuracy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Test Error = 0.338983\n"
     ]
    }
   ],
   "source": [
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "logit = LogisticRegression(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "pipeline = Pipeline(stages = [catIndexer, catOHEncoder, assembler, logit])\n",
    "\n",
    "# Specify parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(logit.regParam , [0.01, 0.05, 0.1, 0.5, 1])\n",
    "            .build())\n",
    "\n",
    "cv = CrossValidator(estimator = pipeline, \n",
    "                    evaluator = evaluator, \n",
    "                    estimatorParamMaps = paramGrid, \n",
    "                    numFolds = 3, \n",
    "                    parallelism = 2, \n",
    "                    seed = 1234)\n",
    "\n",
    "# Run cross-validation on training data, and choose the best set of parameters\n",
    "logitModel = cv.fit(trainDF)\n",
    "\n",
    "# Make predictions on test data. logitModel uses the best model found (regParam = 0.1)\n",
    "log_prediction = logitModel.transform(testDF)\n",
    "result = prediction.select(\"features\", \"label\", \"prediction\").collect()\n",
    "\n",
    "# # Print some predictions\n",
    "# for row in result[0:5]:\n",
    "#     pp.pprint(\"features=%s, label=%s -> prediction=%s\" % \n",
    "#               (row.features, row.label, row.prediction))\n",
    "\n",
    "logit_accuracy = evaluator.evaluate(log_prediction)\n",
    "\n",
    "print(\"Logistic Test Error = %g\" % (1.0 - logit_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "ppgh3DCeuP7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Test Error = 0.338983\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a DecisionTreeClassifier instance. This instance is an Estimator.\n",
    "decision = DecisionTreeClassifier(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "pipeline = Pipeline(stages = [catIndexer, catOHEncoder, assembler, decision])\n",
    "\n",
    "# Specify parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(decision.maxDepth , [2, 5, 10, 20, 30])\n",
    "            .addGrid(decision.maxBins , [10, 20, 40, 80, 100])\n",
    "            .build())\n",
    "\n",
    "cv = CrossValidator(estimator = pipeline, \n",
    "                    evaluator = evaluator, \n",
    "                    estimatorParamMaps = paramGrid, \n",
    "                    numFolds = 3, \n",
    "                    parallelism = 2, \n",
    "                    seed = 1)\n",
    "\n",
    "# Run cross-validation on training data, and choose the best set of parameters\n",
    "decisionModel = cv.fit(trainDF)\n",
    "\n",
    "# Make predictions on test data. Decisionmodel uses the best model found (maxDepth = 0.1)\n",
    "dec_prediction = decisionModel.transform(testDF)\n",
    "result = prediction.select(\"features\", \"label\", \"prediction\").collect()\n",
    "\n",
    "# # Print some predictions\n",
    "# for row in result[0:2]:\n",
    "#     pp.pprint(\"features=%s, label=%s -> prediction=%s\" % \n",
    "#               (row.features, row.label, row.prediction))\n",
    "\n",
    "decision_accuracy = evaluator.evaluate(dec_prediction)\n",
    "\n",
    "print(\"Decision Test Error = %g\" % (1.0 - decision_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Error = 0.372881\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create a RandomForestClassifier instance\n",
    "rforest = RandomForestClassifier(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "# Create a ML pipeline\n",
    "pipeline = Pipeline(stages = [catIndexer, catOHEncoder, assembler, rforest])\n",
    "\n",
    "# Specify parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rforest.maxDepth, [5, 10, 20, 30])\n",
    "            .addGrid(rforest.maxBins, [5, 10, 20, 30])\n",
    "            .addGrid(rforest.numTrees, [5, 10, 20, 30])\n",
    "            .build())\n",
    "\n",
    "cv = CrossValidator(estimator = pipeline, \n",
    "                    evaluator = evaluator, \n",
    "                    estimatorParamMaps = paramGrid, \n",
    "                    numFolds = 3, \n",
    "                    parallelism = 2, \n",
    "                    seed = 1)\n",
    "\n",
    "# Run cross-validation on traning data, and choose the best set of parameters\n",
    "rfModel = cv.fit(trainDF)\n",
    "\n",
    "# Make predictions on test data. RandomForest model use the best model found\n",
    "rf_prediction = rfModel.transform(testDF)\n",
    "result = prediction.select(\"features\", \"label\", \"prediction\").collect()\n",
    "\n",
    "# # Print some predictions\n",
    "# for row in result[0:2]:\n",
    "#     pp.pprint(\"features=%s, label=%s -> prediction=%s\" %\n",
    "#              (row.features, row.label, row.prediction))\n",
    "\n",
    "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
    "\n",
    "print(\"Random Forest Test Error = %g\" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Gradient-boosted tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Classifer Test Error = 0.355932\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create a Gradient-boosted tree classifer instance\n",
    "GBT = GBTClassifier(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "# Create a ML pipeline\n",
    "pipeline = Pipeline(stages = [catIndexer, catOHEncoder, assembler, GBT])\n",
    "\n",
    "# Specify parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(GBT.maxDepth, [2, 5, 10])\n",
    "             .addGrid(GBT.maxBins, [10, 20, 40])\n",
    "             .addGrid(GBT.maxIter, [5, 10, 20])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator = pipeline, \n",
    "                    evaluator = evaluator, \n",
    "                    estimatorParamMaps = paramGrid, \n",
    "                    numFolds = 3, \n",
    "                    parallelism = 2, \n",
    "                    seed = 1)\n",
    "\n",
    "# Run cross-validation on traning data, and choose the best set of parammters\n",
    "GBTclassifer = cv.fit(trainDF)\n",
    "\n",
    "# Make predctions on test data. Gradient-boosted model use the best model found\n",
    "gbt_prediction = GBTclassifer.transform(testDF)\n",
    "result = prediction.select(\"features\", \"label\", \"prediction\").collect()\n",
    "\n",
    "# # Print some predictions\n",
    "# for row in result[0:2]:\n",
    "#     pp.pprint(\"feature=%s | label=%s -> predcition=%s\" %\n",
    "#               (row.features, row.label, row.prediction))\n",
    "    \n",
    "GBT_accuracy = evaluator.evaluate(gbt_prediction)\n",
    "\n",
    "print(\"GBT Classifer Test Error = %g\" % (1.0 - GBT_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# mcpc = MultilayerPerceptronClassifier(featuresCol='features', labelCol='label', \n",
    "#                                       maxIter= 100, layers=[10, 11, 10, 3])\n",
    "\n",
    "# # add parameter grid \n",
    "# paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "# # Create a  MLpipeline\n",
    "# pipeline = Pipeline(stages = [catIndexer, catOHEncoder, assembler, mcpc])\n",
    "\n",
    "# # Setup CrossValidator \n",
    "# # A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# cv = CrossValidator(estimator = pipeline, \n",
    "#                     evaluator = evaluator, \n",
    "#                     estimatorParamMaps = paramGrid, \n",
    "#                     numFolds = 3, \n",
    "#                     parallelism = 2, \n",
    "#                     seed = 1)\n",
    "\n",
    "# # Run cross-validation on training data, and choose the best set of parameters\n",
    "# MLClassifier = cv.fit(trainDF)\n",
    "\n",
    "# # Make predictions on testing data, MultilayerPerceptron model use the best model found\n",
    "# ml_prediction = MLClassifier.transform(testDF)\n",
    "# result = ml_prediction.select(\"features\", \"label\", \"prediction\").collect()\n",
    "\n",
    "# # Print some predictions\n",
    "# for row in result[0:2]:\n",
    "#     pp.pprint(\"features=%s | label=%s -> prediction=%s\" %\n",
    "#               (row.features, row.label, row.prediction))\n",
    "    \n",
    "# MLClassifier_accuracy = evaluator.evaluate(ml_prediciton)\n",
    "\n",
    "# print(\"Multilayer Perceptron Test Error = %g\" % (1.0 - MLClassifier_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6 - NaiveBayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Test Error = 0.305085\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(featuresCol = \"features\", labelCol = \"label\")\n",
    "\n",
    "# Create a pipeline \n",
    "pipeline = Pipeline(stages = [catIndexer, catOHEncoder, assembler, nb])\n",
    "\n",
    "# Specify parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8])\n",
    "            .build())\n",
    "\n",
    "# A cross-validation requires an Estimator,...\n",
    "cv = CrossValidator(estimator = pipeline,\n",
    "                    evaluator = evaluator,\n",
    "                    estimatorParamMaps = paramGrid,\n",
    "                    numFolds = 3,\n",
    "                    parallelism = 2,\n",
    "                    seed = 1)\n",
    "\n",
    "# Run predictions on training data, and choose the best of set parameters\n",
    "NaiveBayes = cv.fit(trainDF)\n",
    "\n",
    "# Make predictions testing data, Naiva Bayes model use the best model found\n",
    "nb_prediction = NaiveBayes.transform(testDF)\n",
    "result = prediction.select(\"features\", \"label\", \"prediction\")\n",
    "\n",
    "# for row in result[0:2]:\n",
    "#     pp.pprint(\"features=%s | label=%s -> prediction=%s\" %\n",
    "#               (row.features, row.label, row.prediction))\n",
    "\n",
    "nb_accuracy = evaluator.evaluate(nb_prediction)\n",
    "\n",
    "print(\"Naive Bayes Test Error = %g\" % (1.0 - nb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Test Error = 0.305085\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "linearSVC = LinearSVC(featuresCol = \"features\", labelCol = \"label\")\n",
    " # Create a ML pipeline \n",
    "pipeline = Pipeline(stages = [catIndexer, catOHEncoder, assembler, linearSVC])\n",
    "\n",
    "# Specify parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(linearSVC.maxIter, [10, 20, 30, 40])\n",
    "            .addGrid(linearSVC.regParam, [0.01, 0.1, 0.5, 1 ])\n",
    "            .build())\n",
    "\n",
    "# Set cross-validation\n",
    "cv = CrossValidator(estimator = pipeline,\n",
    "                    evaluator = evaluator,\n",
    "                    estimatorParamMaps = paramGrid,\n",
    "                    numFolds = 3,\n",
    "                    parallelism = 2,\n",
    "                    seed = 1)\n",
    "\n",
    "# Run prediction on traning data,\n",
    "LinearSVC = cv.fit(trainDF)\n",
    "\n",
    "# Make Prediction on testing data\n",
    "SVC_prediction = NaiveBayes.transform(testDF)\n",
    "result = SVC_prediction.select(\"features\", \"label\", \"prediction\")\n",
    "\n",
    "# # Print some predictions\n",
    "# for row in result[0:2]:\n",
    "#     pp.pprint(\"feature=%s | label=%s -> predcition=%s\" %\n",
    "#               (row.features, row.label, row.prediction))\n",
    "\n",
    "linearSVC_accuracy = evaluator.evaluate(SVC_prediction)\n",
    "\n",
    "print(\"Naive Bayes Test Error = %g\" % (1.0 - linearSVC_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So sánh kết quả của các mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = ['Logistic', 'Decision', 'RF', 'GBT', 'NaiveB', 'LinearSVC']\n",
    "y = [logit_accuracy, decision_accuracy, rf_accuracy, GBT_accuracy, nb_accuracy, linearSVC_accuracy]\n",
    "\n",
    "# Bar chart with day against tip\n",
    "plt.bar(x, y)\n",
    "\n",
    "plt.title(\"Bar Chart\")\n",
    "\n",
    "# Setting the X and Y labels\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Adding the legends\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nhận xét: \n",
    "- Hai mô hình cuối Naive Bayes và Linear SVC cho kết quả tốt hơn các mô hình còn lại. Nhưng nhìn chung, kết quá của tất cả các mô hình chưa được tốt, phần này có liên quan đến xử lí dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0xUhg-duP7u"
   },
   "source": [
    "## Câu 3 - So sánh các mô hình hồi quy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkuIzREduP7v"
   },
   "source": [
    "Thực hiện việc train tất cả các mô hình `LinearRegression`, `DecisionTreeRegression` và `RandomForestRegression`, `GBTRegression` trên tập một tập dữ liệu cho bài toán hồi quy (có 10-100 thuộc tính gồm cả thuộc tính phân loại và thuộc tính số và có 100-1000 đối tượng) mà bạn quan tâm ở [UCI Regression Dataset](https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=10to100&numIns=100to1000&type=&sort=nameUp&view=table). Điều chỉnh các siêu tham số của các mô hình để chọn mô hình tốt nhất dùng cross validation. Để tránh lặp lại các bước xử lý giống nhau nhiều lần như ở trên bạn nên tạo pipeline các bước xử lý. Tham khảo cách tạo pipeline cho mô hình ở [đây](https://spark.apache.org/docs/latest/ml-pipeline.html). Tham khảo document về các classifier của Spark ở [regression module](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "HKUtGbEfuP7v"
   },
   "outputs": [],
   "source": [
    "# Viết code của bạn ở đây\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
